import feedparser
import re
import openpyxl
from subprocess import Popen, call
import holidays as pyholidays
from datetime import datetime, timedelta, date, time
from sqlalchemy import create_engine
from os import listdir, walk, path, remove
from openpyxl.worksheet.datavalidation import DataValidation
from openpyxl.styles import Color, PatternFill
from openpyxl.styles import colors
from sqlalchemy import exc
from pprint import pprint as pprint
from business_duration import businessDuration


class CheckRSSFeeds():

    def __init__(self):

        # location of the database
        self.engine = create_engine('sqlite:///C:\\Users\\Public\\VAS\\VAS.db', echo=False)
        #self.engine = create_engine('sqlite:///Y:\\VAS.db', echo=False)
        self.now = datetime.now()

        # --Change this to check_exel_file_test for testing
        self.file_path = CheckRSSFeeds.check_exel_file(self)
        self.wb = openpyxl.load_workbook(self.file_path, read_only=False, keep_vba=True)
        self.ws = self.wb.active

    def activator(self):
        # activates rss feeds comment out to disable feed
        CheckRSSFeeds.suse(self)
        print('finished SUSE')
        CheckRSSFeeds.centos(self)
        print('finished centos')
        CheckRSSFeeds.debian(self)
        print('finished debian')
        CheckRSSFeeds.redhat(self)
        print('finished redhat')
        # CheckRSSFeeds.ubuntu(self)
        # print('finished ubuntu')
        CheckRSSFeeds.ubuntu2(self)
        print('finished ubuntu2')
        CheckRSSFeeds.cisco(self)
        print('finished cisco')
        CheckRSSFeeds.f5(self)
        print('finished f5')
        CheckRSSFeeds.freebsd(self)
        print('finished freebsd')
        CheckRSSFeeds.jenkins(self)
        print('finished jenkins')
        # CheckRSSFeeds.symantec(self)
        # print('finished symantec')
        # CheckRSSFeeds.juniper(self)
        # print('finished juniper')
        # CheckRSSFeeds.paloalto(self)
        # print('finished paloalto')
        # CheckRSSFeeds.supportcenter(self)
        # print('finished supportcenter')
        CheckRSSFeeds.aruba(self)
        print('finished aruba')
        CheckRSSFeeds.drupalIterator(self)
        print('finished drupal')
        CheckRSSFeeds.export(self)
        print('finished export')

    def suse(self):

        # This code has been replaced it will not be executed
        url = 'https://linuxsecurity.com/advisories/suse?format=feed&type=rss'
        CheckRSSFeeds.parseRss(self, url, 'SUSE', 'title', 'id', 'summary', 'published', 1, '1')

    def centos(self):

        url = 'https://linuxsecurity.com/advisories/centos?format=feed&type=rss'
        CheckRSSFeeds.parseRss(self, url, 'CentOS', 'title', 'id', 'summary', 'published', 1, '1')

    def debian(self):

        url = 'https://linuxsecurity.com/advisories/debian?format=feed&type=rss'
        CheckRSSFeeds.parseRss(self, url, 'Debian', 'title', 'id', 'summary', 'published', 1, '1')

    def redhat(self):

        url = 'https://linuxsecurity.com/advisories/red-hat?format=feed&type=rss'
        CheckRSSFeeds.parseRss(self, url, 'RedHat', 'title', 'id', 'summary', 'published', 1, '1')

    def ubuntu(self):

        url = 'https://linuxsecurity.com/advisories/ubuntu?format=feed&type=rss'
        CheckRSSFeeds.parseRss(self, url, 'Ubuntu', 'title', 'id', 'summary', 'published', 1, '1')

    def ubuntu2(self):

        url = 'https://usn.ubuntu.com/rss.xml'
        CheckRSSFeeds.parseRss(self, url, 'Ubuntu', 'title', 'id', 'summary', 'published', 0, '1')

    # def suseIterator(self):
    #     """ Loops over all suse rss newsfeed url's and calls the main.suse function.
    #     """
    #     suseUrls = ['https://www.novell.com/newsfeeds/rss/patches/patches-suse_appliance_toolkit-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-smt-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-suse_appliance_toolkit-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-sle_desktop-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-sle_high_availability-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-sle_point_of_service-daily-security.xml'
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-sle_real_time-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-sle_server-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-sle_sdk-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-suse_manager-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-suse_cloud-daily-security.xml',
    #                 'https://www.novell.com/newsfeeds/rss/patches/patches-suse_enterprise_storage-daily-security.xml'
    #                 ]
    #     for url in suseUrls:
    #         main.suse(url)
    #
    # def suse(self, url):
    #
    #     url + '?format=feed&type=rss'
    #     CheckRSSFeeds.parseRss(self, url, 'Suse', 'title', 'link', 'summary', 'published', 1, '1')

    def cisco(self):

        url = 'https://tools.cisco.com/security/center/psirtrss20/CiscoSecurityAdvisory.xml'
        CheckRSSFeeds.parseRss(self, url, 'Cisco', 'title', 'link', 'summary', 'published', 3, '1')

    def f5(self):

        url = 'https://api-u.f5.com/support/fedsearch/v2/rss?page=1&results=10&source=kbarticles&source=techcomm&documentType=Security%20Advisory&lastPublishedDateStart=2019-05-28&linkBack=https://support.f5.com/csp/new-updated-articles'
        CheckRSSFeeds.parseRss(self, url, 'F5', 'title', 'id', 'summary', 'published', 0, '1')

    def freebsd(self):

        url = 'https://www.freebsd.org/security/rss.xml'
        CheckRSSFeeds.parseRss(self, url, 'Freebsd', 'title', 'link', 'title', 'published', 0, '2')

    def jenkins(self):

        url = 'https://groups.google.com/forum/feed/jenkinsci-advisories/msgs/atom_v1_0.xml'
        CheckRSSFeeds.parseRss(self, url, 'Jenkins', 'title', 'link', 'summary', 'updated', 0, '2')

    def symantec(self):
        url = 'https://www.symantec.com/xml/rss/listings.jsp?lid=advisories'
        CheckRSSFeeds.parseRss(self, url, 'Symantec', 'title', 'link',
                               'description', 'published', 0, '2')

    def juniper(self):
        url = 'https://kb.juniper.net/InfoCenter/index?page=content&channel=SECURITY_ADVISORIES'
        CheckRSSFeeds.parseRss(self, url, 'Juniper', 'title', 'id', 'title', 'published', 0, '2')

    def paloalto(self):
        url = 'https://securityadvisories.paloaltonetworks.com/(X(1)S(mqx30avvuyvyqmj5ukosuefh))/?AspxAutoDetectCookieSupport=1'
        CheckRSSFeeds.parseRss(self, url, 'PaloAlto', 'title', 'id', 'title', 'published', 0, '2')

    def supportcenter(self):
        url = 'https://supportcenter.checkpoint.com/supportcenter/portal?eventSubmit_doGoviewsecurityalerts=#type=Security+Alerts&product=&version=&severity='
        CheckRSSFeeds.parseRss(self, url, 'PaloAlto', 'title', 'id', 'title', 'published', 0, '2')

    def aruba(self):
        url = 'https://www.arubanetworks.com/security-advisory/feed/'
        CheckRSSFeeds.parseRss(self, url, 'Aruba', 'title', 'id', 'summary', 'published', 0, '2')

    def drupalIterator(self):
        """ Loops over all drupal rss newsfeed url's and calls the main.suse function.
        """
        suseUrls = ['https://www.drupal.org/security/rss.xml',
                    'https://www.drupal.org/security/contrib/rss.xml'
                    ]
        for url in suseUrls:
            main.drupal(url)

    def drupal(self, url):
        url = 'https://www.drupal.org/security/rss.xml'
        CheckRSSFeeds.parseRss(self, url, 'Drupal', 'title', 'link',
                               'description', 'published', 2, '2')

    def parseRss(self, url, site, titlekeyword, linkKeyWord, summaryKeyWord, publishedKeyWord, severityCase, priority):
        """Parses the inserted url. Retrieves link, summary, published date and severity from the newsfeed entries
        and inserts them into the database.
        Attributes:
            url : (String) The url of the rss newsfeed.
            site : (String) The type of the rss newsfeed.
            linkKeyWord : (String) Indicates where the function searches for the link to the entry.
            summaryKeyWord : (String) Indicates where the function searches for the link of the entry.
            publishedKeyWord : (String) Indicates where the function searches for the publish date of the entry.
            severityCase : (Integer) Indicates where the function searches for the severity of the entry.
                    Default: There is no severity given in this newsfeed, so the severity is 'None'.
                    Case 1: The function searches in ['title_detail']['value'] for the severity.
                    Case 2: The function searches in ['title'] for the severity.
            priority : (String) Priority of the rss newsfeed.
        """

        newsfeed = feedparser.parse(url)

        for entry in newsfeed.entries:
            title = entry[titlekeyword]
            link = entry[linkKeyWord]
            summary = entry[summaryKeyWord]
            summary = summary.replace("'", '"')
            published = entry[publishedKeyWord]

            if site == 'Symantec' or site == 'Suse':
                published = datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %z')
                naive = published.replace(tzinfo=None)
                if naive < datetime(2019, 5, 1):
                    continue
            # Retrieve the severity from the value in the title detail of the newsfeed entries
            if severityCase == 1:
                severity = entry['title_detail']['value']

                try:
                    severity = re.search('important|moderate|critical',
                                         severity, re.IGNORECASE).group()
                except AttributeError:
                    severity = 'None'
            elif severityCase == 2:
                severity = entry['title']

                try:
                    severity = re.search(
                        'highly critical|moderately critical|critical', severity, re.IGNORECASE).group()
                except AttributeError:
                    severity = 'None'
            elif severityCase == 3:
                severity = entry['summary_detail']['value']
                try:
                    severity = re.search('Medium|High|Low', severity).group()
                except AttributeError:
                    severity = 'None'
                    # No severity given in the newsfeed entries
            else:
                severity = 'None'

            CheckRSSFeeds.InsertBD(self, site, title, link, summary, published, severity, priority)

    def InsertBD(self, title, site, link, summary, published, severity, priority):

        sql = "INSERT INTO RSSMonitor (TimeOfInsertion, Type, Title, URL, Summery, Published, Severity, Priority) VALUES ('{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}')".format(
            self.now.strftime('%d.%m.%Y %H:%M:%S'), title, site, link, summary, published, severity, priority)
        try:
            self.engine.execute(sql)
            print(sql)
        except exc.SQLAlchemyError:
            print('Duplicate already exists')

    def export(self):

        ws = self.ws

        url_excel = []
        url_db = [row[0] for row in self.engine.execute(
            'SELECT URL FROM RSSMonitor').fetchall()]

        # creates data validation for processor
        dv = DataValidation(type='list', formula1='"Lada, Kamil, Jesper, Cosmin"', allowBlank=True)
        ws.add_data_validation(dv)
        dv.add('J2:J1048576')

        for column in ws.iter_cols(min_row=ws.min_row, max_col=4, min_col=4, max_row=ws.max_row):
            for cell in column:
                if cell.value is not None:
                    url_excel.append(cell.value)

        # TODO add exception if table exist or if sql fails due to wrong parameter.
        if set(url_excel) is not set(url_db):
            difference = set(url_excel).symmetric_difference(set(url_db))
        else:
            print('No new items to be synced')
            quit()
        d = date.today() - timedelta(days=1)
        today = d.strftime('%d.%m.%Y')
        sinc_sql = "SELECT * FROM RSSMonitor WHERE URL in ('{}')".format(
            "', '".join(difference))
        res = [row for row in self.engine.execute(sinc_sql).fetchall()]

        for vul in res:
            vul = list(vul)
            timestamp = vul[0]
            date_timestamp = datetime.strptime(timestamp, '%d.%m.%Y %H:%M:%S')
            date_timestamp = datetime.date(date_timestamp)
            if date_timestamp >= date.today():
                ws.append(vul)

        CheckRSSFeeds.check_violations(self)

    def check_violations(self):

        ws = self.ws
        CheckRSSFeeds.del_none_rows(self)
        # dict is used in a loop to capture improper dates due to user input error

        for row in ws.iter_rows(min_row=2, max_col=ws.max_column, min_col=0, max_row=ws.max_row):
            severity = row[7]
            severity = severity.value
            timestemp_cell = row[0]
            time_of_completion = row[11]
            time_of_assignment = row[8]

            redfill = PatternFill(start_color='FFFF0000', end_color='FFFF0000', fill_type='solid')
            greenfill = PatternFill(start_color='008000', end_color='008000', fill_type='solid')

            if timestemp_cell.value is not None:
                if isinstance(timestemp_cell.value, date) is not True:
                    timestemp = datetime.strptime(timestemp_cell.value, '%d.%m.%Y %H:%M:%S')
                else:
                    timestemp = timestemp_cell.value

            if time_of_completion.value is not None:

                # Not deleted in case regex check of dates is needed in the future
                # pattern = re.compile(r"\d{2}.\d{2}.\d{4}\s\d{1,2}:\d{2}:\d{2}")
                # replacment_dict = {"-": ".", "/": "."}
                # if re.match(pattern, timestemp_cell_str):
                #     print('Regular time given')
                #     print(timestemp_cell_str)
                # else:
                #     print('did not match')
                #     print('--------' + timestemp_cell_str)
                #     for i, j in replacment_dict.items():
                #         timestemp_cell_str = timestemp_cell_str.replace(i, j)
                try:
                    difference = CheckRSSFeeds.seconds_between(self, time_of_completion.value, timestemp)
                    if difference != 'Error in Date':
                        difference = timedelta(seconds=difference)

                        if severity in ['Critical', 'critical', 'Highly critical', 'High']:
                            max_time = timedelta(seconds=14400)
                        elif severity in ['Important', 'important', 'Moderate', 'moderate', 'Moderately critical', 'Medium']:
                            max_time = timedelta(seconds=28800)
                        elif severity in ['low', 'Low', 'None', 'none'] or severity is None:
                            max_time = timedelta(seconds=57600)
                        else:
                            ws.cell(row=timestemp_cell.row, column=13).value = 'Missing Priority'
                            continue
                        if max_time is not None:
                            if difference < max_time:
                                ws.cell(row=timestemp_cell.row, column=13).value = difference
                                ws.cell(row=timestemp_cell.row, column=13).fill = greenfill

                            elif difference > max_time:
                                ws.cell(row=timestemp_cell.row, column=13).value = difference
                                ws.cell(row=timestemp_cell.row, column=13).fill = redfill
                except Exception as e:
                    print(e)
                    print('exception, error in timestamp')
                    ws.cell(row=timestemp_cell.row, column=13).value = 'Error in TimeStamp'

            if time_of_assignment.value is not None:
                try:
                    difference = CheckRSSFeeds.seconds_between(self, time_of_assignment.value, timestemp)
                    if difference != 'Error in Date':
                        difference = timedelta(seconds=difference)

                        if severity in ['Critical', 'critical', 'Highly critical', 'High']:
                            max_time = timedelta(seconds=14400)
                        elif severity in ['Important', 'important', 'Moderate', 'moderate', 'Moderately critical', 'Medium']:
                            max_time = timedelta(seconds=28800)
                        elif severity in ['low', 'Low', 'None', 'none'] or severity is None:
                            max_time = timedelta(seconds=57600)
                        else:
                            ws.cell(row=timestemp_cell.row, column=13).value = 'Missing Priority'
                            continue
                        if max_time is not None:
                            if difference < max_time:
                                ws.cell(row=timestemp_cell.row, column=11).value = difference
                                ws.cell(row=timestemp_cell.row, column=11).fill = greenfill

                            elif difference > max_time:
                                ws.cell(row=timestemp_cell.row, column=11).value = difference
                                ws.cell(row=timestemp_cell.row, column=11).fill = redfill
                except Exception as e:
                    print(e)
                    print('exception, error in timestamp')
                    ws.cell(row=timestemp_cell.row, column=11).value = 'Error in TimeStamp'

        self.wb.save(self.file_path)
        CheckRSSFeeds.backup(self)
        print('Run Finished and saved on ' + self.file_path)

    def check_exel_file(self):

        current_dir_year = self.now.year
        current_dir_month = self.now.month

        current_file = 'VAS_RSS-' + str(current_dir_year) + '-' + str(current_dir_month) + '.xlsm'
        current_file_path = 'S:\\' + current_file

        try:
            existing_dirs = listdir('S:\\')
            if current_file in existing_dirs:
                print('Found Excel ' + current_file)
                return current_file_path
            else:
                print('Could not find file of current month, create new file in sharepoint and start again')
        except Exception as e:
            print(e)
            print('Could not find S: drive (SharePoint drive) make sure network drive is connecteded in file explorer, open internet explorer and log in to SharePoint')
            quit()

    def check_exel_file_test(self):
        return(r'S:\VAS_RSS-2019-7.xlsm')

    def check_previous_excel(self, existing_dirs):
        last_dir = existing_dirs[-1]
        print(last_dir)
        last_dir_path = 'S:\\' + last_dir
        print(last_dir_path)
        self.file_path = last_dir_path
        self.wb = openpyxl.load_workbook(self.last_dir_path, read_only=False, keep_vba=True)
        self.ws = self.wb.active
        CheckRSSFeeds.check_violations(self)

    def backup(self):

        now = self.now
        now = now.strftime('%d_%m_%Y-%H.%M.%S')

        try:
            backups = listdir(r'C:\Users\Public\VAS\RSS_Backup')
            if len(backups) > 150:
                backups.sort()
                remove(r'C:\Users\Public\VAS\RSS_Backup\\' + backups[0])
            self.wb.save(r'C:\Users\Public\VAS\RSS_Backup\RSS' + str(now) + '.xlsm')
            print('Running from local')
        except Exception as e:
            print(e)
            backups = listdir(r'Y:\RSS_Backup')
            if len(backups) > 150:
                backups.sort()
                remove(r'Y:\RSS_Backup\\' + backups[0])
            self.wb.save(r'Y:\RSS_Backup\\' + str(now) + '.xlsm')
            print('Running from remote')

    def seconds_between(self, end_date, start_date):

        # indicate time of working hours
        biz_open_time = time(8, 0, 0)
        biz_close_time = time(17, 0, 0)

        holiday_list = pyholidays.CZ()
        unit = 'sec'
        try:
            difference = businessDuration(startdate=start_date, enddate=end_date, starttime=biz_open_time,
                                          endtime=biz_close_time, holidaylist=holiday_list, unit=unit)
            return(difference)
        except Exception as e:
            print(e)
            return('Error in Date')

    def del_none_rows(self):
        ws = self.ws
        index_row = []

        # loop each row in column A
        for i in range(4, ws.max_row):
            # define emptiness of cell
            if ws.cell(i, 4).value is None:
                # collect indexes of rows
                index_row.append(i)

        # loop each index value
        for row_del in range(len(index_row)):
            ws.delete_rows(idx=index_row[row_del], amount=1)
            # exclude offset of rows through each iteration
            index_row = list(map(lambda k: k - 1, index_row))


if __name__ == '__main__':
    main = CheckRSSFeeds()
    main.check_violations()
